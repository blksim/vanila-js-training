> 웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램이다.
> 이러한 종류의 로봇들은 마치 스스로 마음을 가지고 있는 것처럼 자동으로 웹 사이트들을 탐색하며, 그 방식에 따라 '크롤러', '스파이더', '웜', '봇' 등 각양각색의 이름으로 불린다.(p247)

### 9.1 크롤러와 크롤링
- 웹 크롤러는 특정 웹 사이트가 가리키는 모든 웹 링크를 재귀적으로 따라간다.
- 크롤러의 시작 지점을 '루트 집합'이라고 부른다.
- 크롤링을 진행하면서 웹 페이지 목록은 급격하게 늘어난다.
- 크롤러는 링크 순환에 빠지지 않기 위해 몇 가지 기법을 사용한다.
  1. URL 정규화 : 같은 리소스를 가리키는 중복 URL이 생기지 않도록 정규화한다.
  2. 너비 우선 크롤링 : 깊이 우선 크롤링을 했다가 순환에 걸리기보단, 전체에 걸쳐 URL을 탐색하도록 함으로써 요청을 분산시킬 수 있다.
  3. 스로틀링 : 로봇이 순환을 건드려서 지속적으로 특정 사이트에 접근 시도하면, 일정 시간동안 가져올 수 있는 페이지의 숫자를 제한함으로써 중복 총 횟수를 줄일 수 있다.
  4. URL 크기 제한 : 일정 길이를 초과하는 URL의 크롤링을 제한한다. 하지만 이로 인해 가져오지 못하는 콘텐츠가 발생할 수 있다. 
  5. URL/사이트 블랙리스트 : 크롤러를 순환에 빠지게 하거나 함정인 URL을 따로 블랙리스트로 관리한다.
  6. 패턴 발견 : 몇몇 반복되는 구성요소를 가진 URL을 패턴으로 인식하여 크롤링 대상에서 제외한다.
  7. 콘텐츠 지문(fingerprint) : 콘텐츠에서 체크섬을 얻어내어 이전 체크섬과 비교함으로써 이미 크롤링하고 있는 페이지인지 판단한다.
  8. 사람의 모니터링 : 위의 어떤 휴리스틱으로도 완벽히 걸러지지 않는 함정은 사람의 모니터링을 필요로 한다.
  
  ### 9.2 로봇의 HTTP
  - 로봇도 HTTP 클라이언트와 마찬가지로, HTTP 명세에 맞게 요청해야 한다.
  - 로봇 개발자들이 구현하도록 권장되는 HTTP 헤더는 다음과 같다.(p260)
  - User-Agent : 서버에게 요청한 로봇의 이름을 알려준다
  - From : 로봇의 사용자/관리자 이메일 주소를 제공한다
  - Accept : 서버에게 로봇이 관심있는 콘텐츠 유형이(보내도 되는 타입이 무엇인지) 알려준다
  - Referer : 현재 요청의 URL이 포함된 문서 URL을 제공한다.
  - 몇몇 로봇은 로컬 사본의 유효성을 검사하는 방식과 유사하게 시간이나 엔터티 태그 등을 비교하여 조건부 요청을 보낸다.
  - 개발자들은 가상 호스팅에 대비하여 Host 헤더도 지원해야 한다.
  
  ### 9.3 부적절하게 동작하는 로봇들
   - 폭주하는 로봇 : 일반 유저보다 훨씬 빠르게 HTTP 요청하므로, 순환에 빠지거나 논리적 에러가 있다면 서버를 극심한 부하를 줄 수 있으므로 이에 대한 보호장치가 필수적이다.
   - 오래된 URL : 존재하지 않는 문서에 대한 접근 요청, 에러 페이지를 제공하는 부하로 인해 웹 서버의 수용 능력 감소
   - 길고 잘못된 URL : 순환이나 논리적 오류로 인해 의미없는 URL에 접근할 경우 
   - 호기심이 지나친 로봇 : 크롤링한 데이터에서 사적인 정보를 무시하는 메커니즘이 없을 경우
   - 동적 게이트웨이 접근 : 게이트웨이 애플리케이션 컨텐츠 URL로 요청할 경우.
